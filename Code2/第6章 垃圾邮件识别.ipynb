{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2　特征提取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "# from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1　词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(min_df=1)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out() == (['and', 'document', 'first', 'is', 'one',\n",
    "                                    'second', 'the', 'third', 'this'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorizer.vocabulary_\n",
    "new_vectorizer = CountVectorizer(min_df=1, vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_features=5000\n",
    "max_document_length=100\n",
    "\n",
    "\n",
    "\n",
    "def load_one_file(filename):\n",
    "    x=\"\"\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.strip('\\n')\n",
    "            line = line.strip('\\r')\n",
    "            x+=line\n",
    "    return x\n",
    "\n",
    "def load_files_from_dir(rootdir):\n",
    "    x=[]\n",
    "    list = os.listdir(rootdir)\n",
    "    for i in range(0, len(list)):\n",
    "        path = os.path.join(rootdir, list[i])\n",
    "        if os.path.isfile(path):\n",
    "            v=load_one_file(path)\n",
    "            x.append(v)\n",
    "    return x\n",
    "\n",
    "def load_all_files():\n",
    "    ham=[]\n",
    "    spam=[]\n",
    "    for i in range(1,2):\n",
    "        path=\"../data/mail/enron%d/ham/\" % i\n",
    "        print \"Load %s\" % path\n",
    "        ham+=load_files_from_dir(path)\n",
    "        path=\"../data/mail/enron%d/spam/\" % i\n",
    "        print \"Load %s\" % path\n",
    "        spam+=load_files_from_dir(path)\n",
    "    return ham,spam\n",
    "\n",
    "def get_features_by_wordbag():\n",
    "    ham, spam=load_all_files()\n",
    "    x=ham+spam\n",
    "    y=[0]*len(ham)+[1]*len(spam)\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    print vectorizer\n",
    "    x=vectorizer.fit_transform(x)\n",
    "    x=x.toarray()\n",
    "    return x,y\n",
    "\n",
    "def show_diffrent_max_features():\n",
    "    global max_features\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for i in range(1000,20000,2000):\n",
    "        max_features=i\n",
    "        print \"max_features=%d\" % i\n",
    "        x, y = get_features_by_wordbag()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=0)\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(x_train, y_train)\n",
    "        y_pred = gnb.predict(x_test)\n",
    "        score=metrics.accuracy_score(y_test, y_pred)\n",
    "        a.append(max_features)\n",
    "        b.append(score)\n",
    "        plt.plot(a, b, 'r')\n",
    "    plt.xlabel(\"max_features\")\n",
    "    plt.ylabel(\"metrics.accuracy_score\")\n",
    "    plt.title(\"metrics.accuracy_score VS max_features\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def do_nb_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print \"NB and wordbag\"\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def do_svm_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print \"SVM and wordbag\"\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def get_features_by_wordbag_tfidf():\n",
    "    ham, spam=load_all_files()\n",
    "    x=ham+spam\n",
    "    y=[0]*len(ham)+[1]*len(spam)\n",
    "    vectorizer = CountVectorizer(binary=False,\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    print vectorizer\n",
    "    x=vectorizer.fit_transform(x)\n",
    "    x=x.toarray()\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    print transformer\n",
    "    tfidf = transformer.fit_transform(x)\n",
    "    x = tfidf.toarray()\n",
    "    return  x,y\n",
    "\n",
    "\n",
    "def do_cnn_wordbag(trainX, testX, trainY, testY):\n",
    "    global max_document_length\n",
    "    print \"CNN and tf\"\n",
    "\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,max_document_length], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=1000000, output_dim=128)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"spam\")\n",
    "\n",
    "def do_rnn_wordbag(trainX, testX, trainY, testY):\n",
    "    global max_document_length\n",
    "    print \"RNN and wordbag\"\n",
    "\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Network building\n",
    "    net = tflearn.input_data([None, max_document_length])\n",
    "    net = tflearn.embedding(net, input_dim=10240000, output_dim=128)\n",
    "    net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='categorical_crossentropy')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "              batch_size=10,run_id=\"spm-run\",n_epoch=5)\n",
    "\n",
    "\n",
    "def do_dnn_wordbag(x_train, x_test, y_train, y_testY):\n",
    "    print \"DNN and wordbag\"\n",
    "\n",
    "    # Building deep neural network\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    print  clf\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "def  get_features_by_tf():\n",
    "    global  max_document_length\n",
    "    x=[]\n",
    "    y=[]\n",
    "    ham, spam=load_all_files()\n",
    "    x=ham+spam\n",
    "    y=[0]*len(ham)+[1]*len(spam)\n",
    "    vp=tflearn.data_utils.VocabularyProcessor(max_document_length=max_document_length,\n",
    "                                              min_frequency=0,\n",
    "                                              vocabulary=None,\n",
    "                                              tokenizer_fn=None)\n",
    "    x=vp.fit_transform(x, unused_y=None)\n",
    "    x=np.array(list(x))\n",
    "    return x,y\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print \"Hello spam-mail\"\n",
    "    print \"get_features_by_wordbag\"\n",
    "    x,y=get_features_by_wordbag()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 0)\n",
    "    do_svm_wordbag(x_train, x_test, y_train, y_test)\n",
    "    #do_nb_wordbag(x_train, x_test, y_train, y_test)\n",
    "    #do_dnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    print \"get_features_by_wordbag_tfidf\"\n",
    "    x,y=get_features_by_wordbag_tfidf()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 0)\n",
    "    do_svm_wordbag(x_train, x_test, y_train, y_test)\n",
    "    #do_dnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "    #NB\n",
    "    #do_nb_wordbag(x_train, x_test, y_train, y_test)\n",
    "    #show_diffrent_max_features()\n",
    "\n",
    "    #SVM\n",
    "    #do_svm_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    #DNN\n",
    "    #do_dnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    #print \"get_features_by_tf\"\n",
    "    #x,y=get_features_by_tf()\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 0)\n",
    "    #CNN\n",
    "    #do_cnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "    #RNN\n",
    "    #do_rnn_wordbag(x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
