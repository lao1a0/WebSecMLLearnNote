{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "# from tensorflow.contrib import learn\n",
    "import gensim\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from gensim.models import Doc2Vec\n",
    "# from gensim.models.doc2vec import Doc2Vec,LabeledSentence\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "max_features=400\n",
    "max_document_length=1000\n",
    "vocabulary=None\n",
    "doc2ver_bin=\"doc2ver.bin\"\n",
    "word2ver_bin=\"word2ver.bin\"\n",
    "#LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_one_file(filename):\n",
    "    x=\"\"\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.strip('\\n')\n",
    "            line = line.strip('\\r')\n",
    "            x+=line\n",
    "    f.close()\n",
    "    return x\n",
    "\n",
    "def load_files_from_dir(rootdir):\n",
    "    x=[]\n",
    "    list = os.listdir(rootdir)\n",
    "    for i in range(0, len(list)):\n",
    "        path = os.path.join(rootdir, list[i])\n",
    "        if os.path.isfile(path):\n",
    "            v=load_one_file(path)\n",
    "            x.append(v)\n",
    "    return x\n",
    "\n",
    "def load_all_files():\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    x_test=[]\n",
    "    y_test=[]\n",
    "    path=\"../data/review/aclImdb/train/pos/\"\n",
    "    print \"Load %s\" % path\n",
    "    x_train=load_files_from_dir(path)\n",
    "    y_train=[0]*len(x_train)\n",
    "    path=\"../data/review/aclImdb/train/neg/\"\n",
    "    print \"Load %s\" % path\n",
    "    tmp=load_files_from_dir(path)\n",
    "    y_train+=[1]*len(tmp)\n",
    "    x_train+=tmp\n",
    "\n",
    "    path=\"../data/review/aclImdb/test/pos/\"\n",
    "    print \"Load %s\" % path\n",
    "    x_test=load_files_from_dir(path)\n",
    "    y_test=[0]*len(x_test)\n",
    "    path=\"../data/review/aclImdb/test/neg/\"\n",
    "    print \"Load %s\" % path\n",
    "    tmp=load_files_from_dir(path)\n",
    "    y_test+=[1]*len(tmp)\n",
    "    x_test+=tmp\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def get_features_by_wordbag():\n",
    "    global max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    print vectorizer\n",
    "    x_train=vectorizer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    vocabulary=vectorizer.vocabulary_\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 vocabulary=vocabulary,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    print vectorizer\n",
    "    x_test=vectorizer.fit_transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def show_diffrent_max_features():\n",
    "    global max_features\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for i in range(1000,20000,2000):\n",
    "        max_features=i\n",
    "        print \"max_features=%d\" % i\n",
    "        x, y = get_features_by_wordbag()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=0)\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(x_train, y_train)\n",
    "        y_pred = gnb.predict(x_test)\n",
    "        score=metrics.accuracy_score(y_test, y_pred)\n",
    "        a.append(max_features)\n",
    "        b.append(score)\n",
    "        plt.plot(a, b, 'r')\n",
    "    plt.xlabel(\"max_features\")\n",
    "    plt.ylabel(\"metrics.accuracy_score\")\n",
    "    plt.title(\"metrics.accuracy_score VS max_features\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def do_nb_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print \"NB and wordbag\"\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def do_nb_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    print \"NB and doc2vec\"\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "def do_svm_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print \"SVM and wordbag\"\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "def do_svm_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    print \"SVM and doc2vec\"\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "def do_rf_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    print \"rf and doc2vec\"\n",
    "    clf = RandomForestClassifier(n_estimators=10)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "def get_features_by_wordbag_tfidf():\n",
    "    global max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1,\n",
    "                                 binary=True)\n",
    "    print vectorizer\n",
    "    x_train=vectorizer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    vocabulary=vectorizer.vocabulary_\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 vocabulary=vocabulary,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,binary=True,\n",
    "                                 min_df=1 )\n",
    "    print vectorizer\n",
    "    x_test=vectorizer.fit_transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    x_train=transformer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    x_test=transformer.transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def do_cnn_wordbag(trainX, testX, trainY, testY):\n",
    "    global max_document_length\n",
    "    print \"CNN and tf\"\n",
    "\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,max_document_length], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=1000000, output_dim=128)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"review\")\n",
    "\n",
    "def do_cnn_doc2vec_2d(trainX, testX, trainY, testY):\n",
    "    print \"CNN and doc2vec 2d\"\n",
    "\n",
    "    trainX = trainX.reshape([-1, max_features, max_document_length, 1])\n",
    "    testX = testX.reshape([-1, max_features, max_document_length, 1])\n",
    "\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None, max_features, max_document_length, 1], name='input')\n",
    "    network = conv_2d(network, 16, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 128, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 256, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.01,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit({'input': trainX}, {'target': trainY}, n_epoch=20,\n",
    "               validation_set=({'input': testX}, {'target': testY}),\n",
    "               snapshot_step=100, show_metric=True, run_id='review')\n",
    "\n",
    "\n",
    "def do_cnn_doc2vec(trainX, testX, trainY, testY):\n",
    "    global max_features\n",
    "    print \"CNN and doc2vec\"\n",
    "\n",
    "    #trainX = pad_sequences(trainX, maxlen=max_features, value=0.)\n",
    "    #testX = pad_sequences(testX, maxlen=max_features, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,max_features], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=1000000, output_dim=128,validate_indices=False)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"review\")\n",
    "\n",
    "def do_rnn_wordbag(trainX, testX, trainY, testY):\n",
    "    global max_document_length\n",
    "    print \"RNN and wordbag\"\n",
    "\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Network building\n",
    "    net = tflearn.input_data([None, max_document_length])\n",
    "    net = tflearn.embedding(net, input_dim=10240000, output_dim=128)\n",
    "    net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='categorical_crossentropy')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "              batch_size=10,run_id=\"review\",n_epoch=5)\n",
    "\n",
    "\n",
    "def do_dnn_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print \"MLP and wordbag\"\n",
    "\n",
    "    # Building deep neural network\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    print  clf\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def do_dnn_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    print \"MLP and doc2vec\"\n",
    "    global max_features\n",
    "    # Building deep neural network\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    print  clf\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print metrics.accuracy_score(y_test, y_pred)\n",
    "    print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def  get_features_by_tf():\n",
    "    global  max_document_length\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    vp=tflearn.data_utils.VocabularyProcessor(max_document_length=max_document_length,\n",
    "                                              min_frequency=0,\n",
    "                                              vocabulary=None,\n",
    "                                              tokenizer_fn=None)\n",
    "    x_train=vp.fit_transform(x_train, unused_y=None)\n",
    "    x_train=np.array(list(x_train))\n",
    "\n",
    "    x_test=vp.transform(x_test)\n",
    "    x_test=np.array(list(x_test))\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def cleanText(corpus):\n",
    "    punctuation = \"\"\".,?!:;(){}[]\"\"\"\n",
    "    corpus = [z.lower().replace('\\n', '') for z in corpus]\n",
    "    corpus = [z.replace('<br />', ' ') for z in corpus]\n",
    "\n",
    "    # treat punctuation as individual words\n",
    "    for c in punctuation:\n",
    "        corpus = [z.replace(c, ' %s ' % c) for z in corpus]\n",
    "    corpus = [z.split() for z in corpus]\n",
    "    return corpus\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "\n",
    "    return norm_text\n",
    "\n",
    "def labelizeReviews(reviews, label_type):\n",
    "    labelized = []\n",
    "    for i, v in enumerate(reviews):\n",
    "        label = '%s_%s' % (label_type, i)\n",
    "        #labelized.append(LabeledSentence(v, [label]))\n",
    "        #labelized.append(LabeledSentence(words=v,tags=label))\n",
    "        labelized.append(SentimentDocument(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model.docvecs[z.tags[0]]).reshape((1, size)) for z in corpus]\n",
    "    return np.array(np.concatenate(vecs),dtype='float')\n",
    "\n",
    "\n",
    "def getVecsByWord2Vec(model, corpus, size):\n",
    "    global max_document_length\n",
    "    #x=np.zeros((max_document_length,size),dtype=float, order='C')\n",
    "    x=[]\n",
    "\n",
    "    for text in corpus:\n",
    "        xx = []\n",
    "        for i, vv in enumerate(text):\n",
    "            try:\n",
    "                xx.append(model[vv].reshape((1,size)))\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        x = np.concatenate(xx)\n",
    "\n",
    "    x=np.array(x, dtype='float')\n",
    "    return x\n",
    "\n",
    "\n",
    "def  get_features_by_doc2vec():\n",
    "    global  max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    x_train=cleanText(x_train)\n",
    "    x_test=cleanText(x_test)\n",
    "\n",
    "    x_train = labelizeReviews(x_train, 'TRAIN')\n",
    "    x_test = labelizeReviews(x_test, 'TEST')\n",
    "\n",
    "    x=x_train+x_test\n",
    "    cores=multiprocessing.cpu_count()\n",
    "    #models = [\n",
    "        # PV-DBOW\n",
    "    #    Doc2Vec(dm=0, dbow_words=1, size=200, window=8, min_count=19, iter=10, workers=cores),\n",
    "        # PV-DM w/average\n",
    "    #    Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter=10, workers=cores),\n",
    "    #]\n",
    "    if os.path.exists(doc2ver_bin):\n",
    "        print \"Find cache file %s\" % doc2ver_bin\n",
    "        model=Doc2Vec.load(doc2ver_bin)\n",
    "    else:\n",
    "        model=Doc2Vec(dm=0, size=max_features, negative=5, hs=0, min_count=2, workers=cores,iter=60)\n",
    "\n",
    "\n",
    "        #for model in models:\n",
    "        #    model.build_vocab(x)\n",
    "        model.build_vocab(x)\n",
    "\n",
    "        #models[1].reset_from(models[0])\n",
    "\n",
    "        #for model in models:\n",
    "        #    model.train(x, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        #models[0].train(x, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.train(x, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.save(doc2ver_bin)\n",
    "\n",
    "    #x_test=getVecs(models[0],x_test,max_features)\n",
    "    #x_train=getVecs(models[0],x_train,max_features)\n",
    "    x_test=getVecs(model,x_test,max_features)\n",
    "    x_train=getVecs(model,x_train,max_features)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def  get_features_by_word2vec():\n",
    "    global  max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    x_train=cleanText(x_train)\n",
    "    x_test=cleanText(x_test)\n",
    "\n",
    "    x=x_train+x_test\n",
    "    cores=multiprocessing.cpu_count()\n",
    "\n",
    "    if os.path.exists(word2ver_bin):\n",
    "        print \"Find cache file %s\" % word2ver_bin\n",
    "        model=gensim.models.Word2Vec.load(word2ver_bin)\n",
    "    else:\n",
    "        model=gensim.models.Word2Vec(size=max_features, window=5, min_count=10, iter=10, workers=cores)\n",
    "\n",
    "        model.build_vocab(x)\n",
    "\n",
    "        model.train(x, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.save(word2ver_bin)\n",
    "\n",
    "\n",
    "    x_train=getVecsByWord2Vec(model,x_train,max_features)\n",
    "    x_test = getVecsByWord2Vec(model, x_test, max_features)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print \"Hello review\"\n",
    "    #print \"get_features_by_wordbag_tfidf\"\n",
    "    #x_train, x_test, y_train, y_test=get_features_by_wordbag_tfidf()\n",
    "    #print \"get_features_by_tf\"\n",
    "    #x_train, x_test, y_train, y_test=get_features_by_tf()\n",
    "    #NB\n",
    "    #do_nb_wordbag(x_train, x_test, y_train, y_test)\n",
    "    #SVM\n",
    "    #do_svm_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    #print \"get_features_by_wordbag_tfidf\"\n",
    "    #x,y=get_features_by_wordbag_tfidf()\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 0)\n",
    "\n",
    "    #show_diffrent_max_features()\n",
    "\n",
    "\n",
    "\n",
    "    #DNN\n",
    "    #do_dnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    #print \"get_features_by_tf\"\n",
    "    #x,y=get_features_by_tf()\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 0)\n",
    "    #CNN\n",
    "    #do_cnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "    #RNN\n",
    "    #do_rnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "    #print \"get_features_by_doc2vec\"\n",
    "    x_train, x_test, y_train, y_test=get_features_by_doc2vec()\n",
    "    #print \"get_features_by_word2vec\"\n",
    "    #x_train, x_test, y_train, y_test=get_features_by_word2vec()\n",
    "    #print x_train\n",
    "    #print x_test\n",
    "\n",
    "    #NB\n",
    "    do_nb_doc2vec(x_train, x_test, y_train, y_test)\n",
    "    #CNN\n",
    "\n",
    "    #do_cnn_doc2vec_2d(x_train, x_test, y_train, y_test)\n",
    "    #DNN\n",
    "    #do_dnn_doc2vec(x_train, x_test, y_train, y_test)\n",
    "    #SVM\n",
    "    #do_svm_doc2vec(x_train, x_test, y_train, y_test)\n",
    "    do_rf_doc2vec(x_train, x_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
