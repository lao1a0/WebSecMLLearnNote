{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "# from tensorflow.contrib import learn\n",
    "import gensim\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from gensim.models import Doc2Vec\n",
    "# from gensim.models.doc2vec import Doc2Vec,LabeledSentence\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "max_features=1000\n",
    "max_document_length=160\n",
    "vocabulary=None\n",
    "doc2ver_bin=\"smsdoc2ver.bin\"\n",
    "word2ver_bin=\"smsword2ver.bin\"\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1　数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_files():\n",
    "    x=[]\n",
    "    y=[]\n",
    "    datafile=\"../Data/smsspamcollection/SMSSpamCollection.txt\"\n",
    "    with open(datafile) as f:\n",
    "        for line in f:\n",
    "            line=line.strip('\\n')\n",
    "            label,text=line.split('\\t')\n",
    "            x.append(text)\n",
    "            if label == 'ham':\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_train</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R ü comin back for dinner?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Would you like to see my XXX pics they are so ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dont know you bring some food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haha awesome, I might need to take you up on t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fun fact: although you would think armand woul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>Oic... I saw him too but i tot he din c me... ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>But i'm really really broke oh. No amount is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>Hey Boys. Want hot XXX pics sent direct 2 ur p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>Hi. || Do u want | to join me with sts later? ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3344 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                x_train  y_train\n",
       "0                            R ü comin back for dinner?        0\n",
       "1     Would you like to see my XXX pics they are so ...        1\n",
       "2                         Dont know you bring some food        0\n",
       "3     Haha awesome, I might need to take you up on t...        0\n",
       "4     Fun fact: although you would think armand woul...        0\n",
       "...                                                 ...      ...\n",
       "3339  Oic... I saw him too but i tot he din c me... ...        0\n",
       "3340  But i'm really really broke oh. No amount is t...        0\n",
       "3341  Hey Boys. Want hot XXX pics sent direct 2 ur p...        1\n",
       "3342  U dun say so early hor... U c already then say...        0\n",
       "3343  Hi. || Do u want | to join me with sts later? ...        0\n",
       "\n",
       "[3344 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"x_train\":x_train,\"y_train\":y_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2　特征提取\n",
    "\n",
    "## 8.2.1　词袋和TF-IDF模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(decode_error='ignore', max_features=1000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'000': 0, '04': 1, '0800': 2, '08000839402': 3,\n",
      "                            '08000930705': 4, '0870': 5, '08707509020': 6,\n",
      "                            '08712460324': 7, '10': 8, '100': 9, '1000': 10,\n",
      "                            '10am': 11, '10p': 12, '11': 13, '11mths': 14,\n",
      "                            '12': 15, '12hrs': 16, '1327': 17, '150': 18,\n",
      "                            '150p': 19, '150ppm': 20, '16': 21, '18': 22,\n",
      "                            '1st': 23, '20': 24, '200': 25, '2000': 26,\n",
      "                            '2003': 27, '20p': 28, '25': 29, ...})\n"
     ]
    }
   ],
   "source": [
    "def get_features_by_wordbag():\n",
    "    global max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    print(vectorizer)\n",
    "    x_train=vectorizer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    vocabulary=vectorizer.vocabulary_\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 vocabulary=vocabulary,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    print(vectorizer)\n",
    "    x_test=vectorizer.fit_transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = get_features_by_wordbag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3344 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  990  991  992  \\\n",
       "0       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3339    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3340    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3341    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3342    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3343    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "      993  994  995  996  997  998  999  \n",
       "0       0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "3339    0    0    0    0    0    0    0  \n",
       "3340    0    0    0    0    0    0    0  \n",
       "3341    0    0    0    0    0    0    0  \n",
       "3342    0    0    0    0    0    0    0  \n",
       "3343    0    0    0    0    0    0    0  \n",
       "\n",
       "[3344 rows x 1000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(binary=True, decode_error='ignore', max_features=1000,\n",
      "                stop_words='english', strip_accents='ascii')\n",
      "CountVectorizer(binary=True, decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '02': 2, '03': 3, '04': 4,\n",
      "                            '06': 5, '0800': 6, '08000839402': 7,\n",
      "                            '08000930705': 8, '0870': 9, '08707509020': 10,\n",
      "                            '08712300220': 11, '10': 12, '100': 13, '1000': 14,\n",
      "                            '10p': 15, '11': 16, '12': 17, '12hrs': 18,\n",
      "                            '1327': 19, '150': 20, '150p': 21, '150ppm': 22,\n",
      "                            '16': 23, '18': 24, '1st': 25, '20': 26, '200': 27,\n",
      "                            '2000': 28, '2003': 29, ...})\n"
     ]
    }
   ],
   "source": [
    "def get_features_by_wordbag_tfidf():\n",
    "    global max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1,\n",
    "                                 binary=True)\n",
    "    print(vectorizer)\n",
    "    x_train=vectorizer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    vocabulary=vectorizer.vocabulary_\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 vocabulary=vocabulary,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,binary=True,\n",
    "                                 min_df=1 )\n",
    "    print(vectorizer)\n",
    "    x_test=vectorizer.fit_transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    x_train=transformer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    x_test=transformer.transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_features_by_wordbag_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3344 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  990  991  992  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3339  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3340  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3341  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3342  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3343  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      993  994  995  996  997  998  999  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "3339  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3340  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3341  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3342  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3343  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[3344 rows x 1000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.2　词汇表模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def get_features_by_tf():\n",
    "    global max_document_length\n",
    "    x_train, x_test, y_train, y_test = load_all_files()\n",
    "\n",
    "    # 创建一个 Tokenizer 对象\n",
    "    tokenizer = Tokenizer(num_words=max_document_length, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    \n",
    "    # 将文本转换为整数序列\n",
    "    x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "    x_test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "    \n",
    "    # 将序列填充到相同的长度\n",
    "    x_train_padded = pad_sequences(x_train_sequences, maxlen=max_document_length, padding='post')\n",
    "    x_test_padded = pad_sequences(x_test_sequences, maxlen=max_document_length, padding='post')\n",
    "\n",
    "    return x_train_padded, x_test_padded, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_features_by_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>97</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>111</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>50</td>\n",
       "      <td>137</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3344 rows × 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  150  151  152  \\\n",
       "0       1   13   14    1    3    1    1   14   97   34  ...    0    0    0   \n",
       "1      98   12    1    1    1    5    1    8    1    1  ...    0    0    0   \n",
       "2       1    7    1    1   89    1   74    2    1    1  ...    0    0    0   \n",
       "3       2   68    1    1   61    1    1    2   68   26  ...    0    0    0   \n",
       "4       1   39    6    1   10    1    7    1    1   25  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3339  107    1   10    1    1    1    0    0    0    0  ...    0    0    0   \n",
       "3340    1   93    1  103    1    1    1    3    1    3  ...    0    0    0   \n",
       "3341   38    1    1    2  111   65    1    1    3    1  ...    0    0    0   \n",
       "3342    1    1    8    1    1  127   39    1    1   15  ...    0    0    0   \n",
       "3343   50  137   88    5    1    4    1   49    1    1  ...    0    0    0   \n",
       "\n",
       "      153  154  155  156  157  158  159  \n",
       "0       0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "3339    0    0    0    0    0    0    0  \n",
       "3340    0    0    0    0    0    0    0  \n",
       "3341    0    0    0    0    0    0    0  \n",
       "3342    0    0    0    0    0    0    0  \n",
       "3343    0    0    0    0    0    0    0  \n",
       "\n",
       "[3344 rows x 160 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.3　Word2Vec模型和Doc2Vec模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(corpus):\n",
    "    punctuation = \"\"\".,?!:;(){}[]\"\"\"\n",
    "    corpus = [z.lower().replace('\\n', '') for z in corpus]\n",
    "    corpus = [z.replace('<br />', ' ') for z in corpus]\n",
    "\n",
    "    # treat punctuation as individual words\n",
    "    for c in punctuation:\n",
    "        corpus = [z.replace(c, ' %s ' % c) for z in corpus]\n",
    "    corpus = [z.split() for z in corpus]\n",
    "    return corpus\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "\n",
    "    return norm_text\n",
    "\n",
    "def labelizeReviews(reviews, label_type):\n",
    "    labelized = []\n",
    "    for i, v in enumerate(reviews):\n",
    "        label = '%s_%s' % (label_type, i)\n",
    "        #labelized.append(LabeledSentence(v, [label]))\n",
    "        #labelized.append(LabeledSentence(words=v,tags=label))\n",
    "        labelized.append(SentimentDocument(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model.dv[z.tags[0]]).reshape((1, size)) for z in corpus]\n",
    "    return np.array(np.concatenate(vecs),dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find cache file smsdoc2ver.bin\n"
     ]
    }
   ],
   "source": [
    "def  get_features_by_doc2vec():\n",
    "    global  max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    x_train=cleanText(x_train)\n",
    "    x_test=cleanText(x_test)\n",
    "\n",
    "    x_train = labelizeReviews(x_train, 'TRAIN')\n",
    "    x_test = labelizeReviews(x_test, 'TEST')\n",
    "\n",
    "    x=x_train+x_test\n",
    "    cores=multiprocessing.cpu_count()\n",
    "    #models = [\n",
    "        # PV-DBOW\n",
    "    #    Doc2Vec(dm=0, dbow_words=1, size=200, window=8, min_count=19, iter=10, workers=cores),\n",
    "        # PV-DM w/average\n",
    "    #    Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter=10, workers=cores),\n",
    "    #]\n",
    "    if os.path.exists(doc2ver_bin):\n",
    "        print(\"Find cache file %s\" % doc2ver_bin)\n",
    "        model=Doc2Vec.load(doc2ver_bin)\n",
    "    else:\n",
    "        model=Doc2Vec(dm=0, vector_size=max_features, negative=5, hs=0, min_count=2, workers=cores,epochs=60)\n",
    "        model.build_vocab(x)\n",
    "        model.train(x, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        # model.save((doc2ver_bin)\n",
    "\n",
    "    x_test=getVecs(model,x_test,max_features)\n",
    "    x_train=getVecs(model,x_train,max_features)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = get_features_by_doc2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004458</td>\n",
       "      <td>-0.061380</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>0.035035</td>\n",
       "      <td>0.053963</td>\n",
       "      <td>-0.035363</td>\n",
       "      <td>-0.020818</td>\n",
       "      <td>-0.003369</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.029124</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>-0.042397</td>\n",
       "      <td>0.049348</td>\n",
       "      <td>-0.001347</td>\n",
       "      <td>0.089229</td>\n",
       "      <td>-0.041516</td>\n",
       "      <td>-0.015124</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>-0.042357</td>\n",
       "      <td>-0.027389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070235</td>\n",
       "      <td>-0.152911</td>\n",
       "      <td>-0.094858</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>-0.030888</td>\n",
       "      <td>0.108002</td>\n",
       "      <td>-0.055896</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>-0.029024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102553</td>\n",
       "      <td>-0.020463</td>\n",
       "      <td>0.076963</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.083373</td>\n",
       "      <td>0.012041</td>\n",
       "      <td>-0.046332</td>\n",
       "      <td>-0.018365</td>\n",
       "      <td>-0.105313</td>\n",
       "      <td>-0.185344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008281</td>\n",
       "      <td>-0.041929</td>\n",
       "      <td>-0.007167</td>\n",
       "      <td>0.007217</td>\n",
       "      <td>-0.004693</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.069734</td>\n",
       "      <td>-0.035124</td>\n",
       "      <td>0.065529</td>\n",
       "      <td>0.037060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030794</td>\n",
       "      <td>-0.087129</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>0.016512</td>\n",
       "      <td>0.127887</td>\n",
       "      <td>-0.043624</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>0.042212</td>\n",
       "      <td>-0.023931</td>\n",
       "      <td>-0.013367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001964</td>\n",
       "      <td>-0.063209</td>\n",
       "      <td>0.210218</td>\n",
       "      <td>0.022131</td>\n",
       "      <td>-0.007714</td>\n",
       "      <td>-0.055662</td>\n",
       "      <td>-0.123557</td>\n",
       "      <td>0.114349</td>\n",
       "      <td>0.117815</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010059</td>\n",
       "      <td>-0.083258</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.165079</td>\n",
       "      <td>-0.004385</td>\n",
       "      <td>-0.060333</td>\n",
       "      <td>0.030876</td>\n",
       "      <td>0.066477</td>\n",
       "      <td>-0.005818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028132</td>\n",
       "      <td>-0.084640</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>-0.023565</td>\n",
       "      <td>-0.048779</td>\n",
       "      <td>-0.008749</td>\n",
       "      <td>-0.074701</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>0.090763</td>\n",
       "      <td>-0.013062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011764</td>\n",
       "      <td>-0.026315</td>\n",
       "      <td>0.081805</td>\n",
       "      <td>0.087292</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>-0.033216</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.056211</td>\n",
       "      <td>-0.047910</td>\n",
       "      <td>-0.017038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>-0.103747</td>\n",
       "      <td>-0.158253</td>\n",
       "      <td>-0.045452</td>\n",
       "      <td>0.019096</td>\n",
       "      <td>-0.140323</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>-0.069634</td>\n",
       "      <td>-0.058296</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>-0.038295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057565</td>\n",
       "      <td>-0.136589</td>\n",
       "      <td>0.135482</td>\n",
       "      <td>0.157111</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>-0.052858</td>\n",
       "      <td>0.033509</td>\n",
       "      <td>0.104385</td>\n",
       "      <td>-0.079987</td>\n",
       "      <td>-0.032693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>-0.050124</td>\n",
       "      <td>-0.074816</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>-0.066748</td>\n",
       "      <td>-0.067258</td>\n",
       "      <td>-0.014793</td>\n",
       "      <td>-0.059085</td>\n",
       "      <td>0.051038</td>\n",
       "      <td>0.035934</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030919</td>\n",
       "      <td>-0.045536</td>\n",
       "      <td>0.075405</td>\n",
       "      <td>0.020923</td>\n",
       "      <td>0.106843</td>\n",
       "      <td>-0.018137</td>\n",
       "      <td>0.062117</td>\n",
       "      <td>0.041567</td>\n",
       "      <td>-0.164500</td>\n",
       "      <td>-0.105757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>-0.051853</td>\n",
       "      <td>-0.071851</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.019652</td>\n",
       "      <td>-0.039451</td>\n",
       "      <td>-0.002620</td>\n",
       "      <td>-0.067998</td>\n",
       "      <td>-0.054904</td>\n",
       "      <td>0.125222</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>-0.108308</td>\n",
       "      <td>0.067440</td>\n",
       "      <td>0.080105</td>\n",
       "      <td>0.065208</td>\n",
       "      <td>-0.034076</td>\n",
       "      <td>-0.019555</td>\n",
       "      <td>0.106129</td>\n",
       "      <td>-0.048963</td>\n",
       "      <td>0.015892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>0.010649</td>\n",
       "      <td>-0.085896</td>\n",
       "      <td>-0.022093</td>\n",
       "      <td>0.093712</td>\n",
       "      <td>0.012454</td>\n",
       "      <td>-0.015241</td>\n",
       "      <td>0.023913</td>\n",
       "      <td>-0.054980</td>\n",
       "      <td>0.126428</td>\n",
       "      <td>0.063123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055459</td>\n",
       "      <td>-0.065124</td>\n",
       "      <td>0.114499</td>\n",
       "      <td>0.105787</td>\n",
       "      <td>0.119010</td>\n",
       "      <td>0.028899</td>\n",
       "      <td>0.030762</td>\n",
       "      <td>0.072743</td>\n",
       "      <td>-0.029420</td>\n",
       "      <td>0.011222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>-0.056491</td>\n",
       "      <td>-0.017528</td>\n",
       "      <td>0.070683</td>\n",
       "      <td>-0.052898</td>\n",
       "      <td>-0.003096</td>\n",
       "      <td>-0.004429</td>\n",
       "      <td>-0.087931</td>\n",
       "      <td>-0.042361</td>\n",
       "      <td>0.149701</td>\n",
       "      <td>0.014162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053367</td>\n",
       "      <td>-0.044360</td>\n",
       "      <td>0.065563</td>\n",
       "      <td>0.051504</td>\n",
       "      <td>0.094080</td>\n",
       "      <td>0.014979</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.028915</td>\n",
       "      <td>-0.026320</td>\n",
       "      <td>0.025172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3344 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.004458 -0.061380  0.004328  0.035035  0.053963 -0.035363 -0.020818   \n",
       "1     0.070235 -0.152911 -0.094858  0.056680  0.008789 -0.030888  0.108002   \n",
       "2     0.008281 -0.041929 -0.007167  0.007217 -0.004693  0.000032  0.069734   \n",
       "3    -0.001964 -0.063209  0.210218  0.022131 -0.007714 -0.055662 -0.123557   \n",
       "4    -0.028132 -0.084640  0.004073 -0.023565 -0.048779 -0.008749 -0.074701   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3339 -0.103747 -0.158253 -0.045452  0.019096 -0.140323  0.023600 -0.069634   \n",
       "3340 -0.050124 -0.074816 -0.078068  0.002041 -0.066748 -0.067258 -0.014793   \n",
       "3341 -0.051853 -0.071851 -0.057251 -0.019652 -0.039451 -0.002620 -0.067998   \n",
       "3342  0.010649 -0.085896 -0.022093  0.093712  0.012454 -0.015241  0.023913   \n",
       "3343 -0.056491 -0.017528  0.070683 -0.052898 -0.003096 -0.004429 -0.087931   \n",
       "\n",
       "           7         8         9    ...       990       991       992  \\\n",
       "0    -0.003369  0.043700  0.029124  ... -0.003264 -0.042397  0.049348   \n",
       "1    -0.055896  0.005362 -0.029024  ...  0.102553 -0.020463  0.076963   \n",
       "2    -0.035124  0.065529  0.037060  ...  0.030794 -0.087129  0.092017   \n",
       "3     0.114349  0.117815  0.018036  ... -0.010059 -0.083258  0.000747   \n",
       "4    -0.021888  0.090763 -0.013062  ... -0.011764 -0.026315  0.081805   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3339 -0.058296  0.146667 -0.038295  ...  0.057565 -0.136589  0.135482   \n",
       "3340 -0.059085  0.051038  0.035934  ... -0.030919 -0.045536  0.075405   \n",
       "3341 -0.054904  0.125222  0.007532  ...  0.019306 -0.108308  0.067440   \n",
       "3342 -0.054980  0.126428  0.063123  ...  0.055459 -0.065124  0.114499   \n",
       "3343 -0.042361  0.149701  0.014162  ... -0.053367 -0.044360  0.065563   \n",
       "\n",
       "           993       994       995       996       997       998       999  \n",
       "0    -0.001347  0.089229 -0.041516 -0.015124  0.056957 -0.042357 -0.027389  \n",
       "1     0.001763  0.083373  0.012041 -0.046332 -0.018365 -0.105313 -0.185344  \n",
       "2     0.016512  0.127887 -0.043624 -0.014526  0.042212 -0.023931 -0.013367  \n",
       "3     0.032300  0.165079 -0.004385 -0.060333  0.030876  0.066477 -0.005818  \n",
       "4     0.087292  0.054000 -0.033216  0.014870  0.056211 -0.047910 -0.017038  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3339  0.157111  0.073640 -0.052858  0.033509  0.104385 -0.079987 -0.032693  \n",
       "3340  0.020923  0.106843 -0.018137  0.062117  0.041567 -0.164500 -0.105757  \n",
       "3341  0.080105  0.065208 -0.034076 -0.019555  0.106129 -0.048963  0.015892  \n",
       "3342  0.105787  0.119010  0.028899  0.030762  0.072743 -0.029420  0.011222  \n",
       "3343  0.051504  0.094080  0.014979  0.004061  0.028915 -0.026320  0.025172  \n",
       "\n",
       "[3344 rows x 1000 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def buildWordVector(model, text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            # 使用方括号访问词向量\n",
    "            vec += model.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_features_by_word2vec():\n",
    "    global  max_features\n",
    "    global word2ver_bin\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    x_train=cleanText(x_train)\n",
    "    x_test=cleanText(x_test)\n",
    "\n",
    "    x=x_train+x_test\n",
    "    cores=multiprocessing.cpu_count()\n",
    "\n",
    "    if os.path.exists(word2ver_bin):\n",
    "        print(\"Find cache file %s\" % word2ver_bin)\n",
    "        model=gensim.models.Word2Vec.load(word2ver_bin)\n",
    "    else:\n",
    "        model=gensim.models.Word2Vec(vector_size=max_features, window=10, min_count=1, epochs=60, workers=cores)\n",
    "\n",
    "        model.build_vocab(x)\n",
    "\n",
    "        model.train(x, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        # model.save(word2ver_bin)\n",
    "\n",
    "    # 将组成短信的单词和字符的Word2Vec相加并取平均值\n",
    "    x_train= np.concatenate([buildWordVector(model,z, max_features) for z in x_train])\n",
    "    x_train = scale(x_train)\n",
    "    x_test= np.concatenate([buildWordVector(model,z, max_features) for z in x_test])\n",
    "    x_test = scale(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = get_features_by_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([[1],[3],[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.33630621,  0.26726124,  1.06904497])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale(np.concatenate([[1],[3],[4]])) \n",
    "\n",
    "# scale 函数会将数据的每个特征（列）减去其均值并除以其标准差，从而使得每个特征的均值为 0，标准差为 1\n",
    "# 使用scale函数的作用是，避免多维数据中个别维度的数据过大或者过小从而影响算法分类效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.766432</td>\n",
       "      <td>-0.085114</td>\n",
       "      <td>0.794647</td>\n",
       "      <td>1.581439</td>\n",
       "      <td>0.249115</td>\n",
       "      <td>-0.177475</td>\n",
       "      <td>1.191238</td>\n",
       "      <td>0.355237</td>\n",
       "      <td>-0.025186</td>\n",
       "      <td>-0.498360</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.370850</td>\n",
       "      <td>1.265424</td>\n",
       "      <td>1.112254</td>\n",
       "      <td>1.180773</td>\n",
       "      <td>1.023409</td>\n",
       "      <td>-0.306360</td>\n",
       "      <td>1.072285</td>\n",
       "      <td>0.650548</td>\n",
       "      <td>1.017748</td>\n",
       "      <td>0.190572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.888473</td>\n",
       "      <td>1.407544</td>\n",
       "      <td>-2.178053</td>\n",
       "      <td>-1.218416</td>\n",
       "      <td>0.277493</td>\n",
       "      <td>0.934880</td>\n",
       "      <td>-0.873954</td>\n",
       "      <td>0.088817</td>\n",
       "      <td>-0.952073</td>\n",
       "      <td>-0.720517</td>\n",
       "      <td>...</td>\n",
       "      <td>2.135167</td>\n",
       "      <td>0.065429</td>\n",
       "      <td>-0.724880</td>\n",
       "      <td>-0.843104</td>\n",
       "      <td>-0.583777</td>\n",
       "      <td>0.622140</td>\n",
       "      <td>-3.013898</td>\n",
       "      <td>0.104693</td>\n",
       "      <td>0.623398</td>\n",
       "      <td>0.321471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.952926</td>\n",
       "      <td>-0.112136</td>\n",
       "      <td>-0.902263</td>\n",
       "      <td>-1.046700</td>\n",
       "      <td>1.357294</td>\n",
       "      <td>0.884033</td>\n",
       "      <td>-1.056025</td>\n",
       "      <td>-0.860502</td>\n",
       "      <td>-0.026913</td>\n",
       "      <td>0.352280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773565</td>\n",
       "      <td>1.694021</td>\n",
       "      <td>0.823311</td>\n",
       "      <td>-0.407370</td>\n",
       "      <td>0.539935</td>\n",
       "      <td>-0.665450</td>\n",
       "      <td>-0.516967</td>\n",
       "      <td>-1.283426</td>\n",
       "      <td>-0.243830</td>\n",
       "      <td>0.984675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.499880</td>\n",
       "      <td>-0.568623</td>\n",
       "      <td>-0.580462</td>\n",
       "      <td>-0.554188</td>\n",
       "      <td>0.696698</td>\n",
       "      <td>0.166875</td>\n",
       "      <td>0.103681</td>\n",
       "      <td>0.368997</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>-0.102114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836488</td>\n",
       "      <td>0.175721</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>1.154574</td>\n",
       "      <td>-1.202631</td>\n",
       "      <td>-0.760085</td>\n",
       "      <td>-0.238016</td>\n",
       "      <td>-0.451676</td>\n",
       "      <td>-0.447109</td>\n",
       "      <td>-0.160259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.260642</td>\n",
       "      <td>3.510728</td>\n",
       "      <td>-1.663179</td>\n",
       "      <td>-0.939204</td>\n",
       "      <td>0.054370</td>\n",
       "      <td>-0.391761</td>\n",
       "      <td>2.957015</td>\n",
       "      <td>1.749006</td>\n",
       "      <td>1.609943</td>\n",
       "      <td>0.521986</td>\n",
       "      <td>...</td>\n",
       "      <td>2.346884</td>\n",
       "      <td>0.110440</td>\n",
       "      <td>-1.837250</td>\n",
       "      <td>-2.013840</td>\n",
       "      <td>1.632917</td>\n",
       "      <td>1.789204</td>\n",
       "      <td>-2.427666</td>\n",
       "      <td>-0.296104</td>\n",
       "      <td>0.941223</td>\n",
       "      <td>-1.326385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>-0.348794</td>\n",
       "      <td>0.677643</td>\n",
       "      <td>1.205271</td>\n",
       "      <td>1.694639</td>\n",
       "      <td>-0.383443</td>\n",
       "      <td>-0.520986</td>\n",
       "      <td>-0.426823</td>\n",
       "      <td>0.780282</td>\n",
       "      <td>-1.032909</td>\n",
       "      <td>-0.701918</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.675168</td>\n",
       "      <td>1.880448</td>\n",
       "      <td>0.641596</td>\n",
       "      <td>1.873329</td>\n",
       "      <td>0.297619</td>\n",
       "      <td>-0.574747</td>\n",
       "      <td>0.386792</td>\n",
       "      <td>1.027732</td>\n",
       "      <td>0.825089</td>\n",
       "      <td>0.784090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>2.042167</td>\n",
       "      <td>-0.500722</td>\n",
       "      <td>1.688361</td>\n",
       "      <td>1.750509</td>\n",
       "      <td>1.238162</td>\n",
       "      <td>0.148662</td>\n",
       "      <td>-0.099501</td>\n",
       "      <td>0.905543</td>\n",
       "      <td>2.124442</td>\n",
       "      <td>-1.030257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638152</td>\n",
       "      <td>1.220170</td>\n",
       "      <td>0.848315</td>\n",
       "      <td>0.497110</td>\n",
       "      <td>-0.684422</td>\n",
       "      <td>1.175211</td>\n",
       "      <td>0.067510</td>\n",
       "      <td>1.597693</td>\n",
       "      <td>-0.433835</td>\n",
       "      <td>-0.157550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>-0.035013</td>\n",
       "      <td>0.530499</td>\n",
       "      <td>-1.157368</td>\n",
       "      <td>-0.474230</td>\n",
       "      <td>0.728233</td>\n",
       "      <td>0.936141</td>\n",
       "      <td>-1.492215</td>\n",
       "      <td>0.151246</td>\n",
       "      <td>0.750764</td>\n",
       "      <td>1.101072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294613</td>\n",
       "      <td>0.970884</td>\n",
       "      <td>0.272533</td>\n",
       "      <td>-0.471355</td>\n",
       "      <td>1.453732</td>\n",
       "      <td>0.143975</td>\n",
       "      <td>-0.069851</td>\n",
       "      <td>-0.420969</td>\n",
       "      <td>-0.624299</td>\n",
       "      <td>0.306470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>-0.301434</td>\n",
       "      <td>-0.153360</td>\n",
       "      <td>1.337651</td>\n",
       "      <td>0.207332</td>\n",
       "      <td>-0.323442</td>\n",
       "      <td>-1.167561</td>\n",
       "      <td>-0.362915</td>\n",
       "      <td>-1.011502</td>\n",
       "      <td>-0.479157</td>\n",
       "      <td>-0.749759</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.914934</td>\n",
       "      <td>-0.650348</td>\n",
       "      <td>0.026091</td>\n",
       "      <td>-0.431718</td>\n",
       "      <td>0.049124</td>\n",
       "      <td>0.521676</td>\n",
       "      <td>1.406654</td>\n",
       "      <td>0.035817</td>\n",
       "      <td>-0.503885</td>\n",
       "      <td>-0.366673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>0.565489</td>\n",
       "      <td>-0.606872</td>\n",
       "      <td>0.255719</td>\n",
       "      <td>0.497276</td>\n",
       "      <td>-0.691318</td>\n",
       "      <td>-1.028808</td>\n",
       "      <td>-0.666938</td>\n",
       "      <td>-0.161264</td>\n",
       "      <td>-0.231964</td>\n",
       "      <td>0.310608</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330895</td>\n",
       "      <td>1.940446</td>\n",
       "      <td>0.720022</td>\n",
       "      <td>0.640404</td>\n",
       "      <td>0.919724</td>\n",
       "      <td>-0.841488</td>\n",
       "      <td>-0.785519</td>\n",
       "      <td>0.662721</td>\n",
       "      <td>0.881873</td>\n",
       "      <td>0.202695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3344 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.766432 -0.085114  0.794647  1.581439  0.249115 -0.177475  1.191238   \n",
       "1    -0.888473  1.407544 -2.178053 -1.218416  0.277493  0.934880 -0.873954   \n",
       "2    -0.952926 -0.112136 -0.902263 -1.046700  1.357294  0.884033 -1.056025   \n",
       "3     0.499880 -0.568623 -0.580462 -0.554188  0.696698  0.166875  0.103681   \n",
       "4     1.260642  3.510728 -1.663179 -0.939204  0.054370 -0.391761  2.957015   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3339 -0.348794  0.677643  1.205271  1.694639 -0.383443 -0.520986 -0.426823   \n",
       "3340  2.042167 -0.500722  1.688361  1.750509  1.238162  0.148662 -0.099501   \n",
       "3341 -0.035013  0.530499 -1.157368 -0.474230  0.728233  0.936141 -1.492215   \n",
       "3342 -0.301434 -0.153360  1.337651  0.207332 -0.323442 -1.167561 -0.362915   \n",
       "3343  0.565489 -0.606872  0.255719  0.497276 -0.691318 -1.028808 -0.666938   \n",
       "\n",
       "           7         8         9    ...       990       991       992  \\\n",
       "0     0.355237 -0.025186 -0.498360  ... -1.370850  1.265424  1.112254   \n",
       "1     0.088817 -0.952073 -0.720517  ...  2.135167  0.065429 -0.724880   \n",
       "2    -0.860502 -0.026913  0.352280  ...  0.773565  1.694021  0.823311   \n",
       "3     0.368997  0.267327 -0.102114  ...  0.836488  0.175721 -0.000370   \n",
       "4     1.749006  1.609943  0.521986  ...  2.346884  0.110440 -1.837250   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3339  0.780282 -1.032909 -0.701918  ... -1.675168  1.880448  0.641596   \n",
       "3340  0.905543  2.124442 -1.030257  ...  0.638152  1.220170  0.848315   \n",
       "3341  0.151246  0.750764  1.101072  ...  0.294613  0.970884  0.272533   \n",
       "3342 -1.011502 -0.479157 -0.749759  ... -0.914934 -0.650348  0.026091   \n",
       "3343 -0.161264 -0.231964  0.310608  ... -0.330895  1.940446  0.720022   \n",
       "\n",
       "           993       994       995       996       997       998       999  \n",
       "0     1.180773  1.023409 -0.306360  1.072285  0.650548  1.017748  0.190572  \n",
       "1    -0.843104 -0.583777  0.622140 -3.013898  0.104693  0.623398  0.321471  \n",
       "2    -0.407370  0.539935 -0.665450 -0.516967 -1.283426 -0.243830  0.984675  \n",
       "3     1.154574 -1.202631 -0.760085 -0.238016 -0.451676 -0.447109 -0.160259  \n",
       "4    -2.013840  1.632917  1.789204 -2.427666 -0.296104  0.941223 -1.326385  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3339  1.873329  0.297619 -0.574747  0.386792  1.027732  0.825089  0.784090  \n",
       "3340  0.497110 -0.684422  1.175211  0.067510  1.597693 -0.433835 -0.157550  \n",
       "3341 -0.471355  1.453732  0.143975 -0.069851 -0.420969 -0.624299  0.306470  \n",
       "3342 -0.431718  0.049124  0.521676  1.406654  0.035817 -0.503885 -0.366673  \n",
       "3343  0.640404  0.919724 -0.841488 -0.785519  0.662721  0.881873  0.202695  \n",
       "\n",
       "[3344 rows x 1000 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成的结果中有负数是正常的，这是因为词向量（word vectors）可以是多维空间中的任何实数向量，实数包括正数、负数和零。词向量是通过训练模型（如Word2Vec）学习得到的，它们捕捉了词汇的语义信息。在这些向量空间中，数值的正负并不具有直观的语义意义，但它们可以影响向量之间的相似度计算（如余弦相似度）。\n",
    "\n",
    "关于您提供的 `DataFrame` 中每一行每一列的含义：\n",
    "\n",
    "1. **每一行**：代表一个文档（或文本样本）的向量表示。在自然语言处理中，一个文档可以被转换成一个固定长度的向量，以便于模型处理。这个向量是通过某种方法（如词袋模型、TF-IDF 或词嵌入）将文档中的单词转换为数值表示。\n",
    "\n",
    "2. **每一列**：代表向量中的一个维度。在词向量模型中，每个维度可以捕捉到词汇的不同语义属性。例如，一个维度可能与性别相关，另一个维度可能与情感极性相关。但是，这些维度的具体含义通常是不可解释的，因为它们是模型自动学习得到的。\n",
    "\n",
    "在您的例子中，如果 `get_features_by_word2vec()` 函数的目的是将文档转换为词向量表示，那么：\n",
    "\n",
    "- `x_train` 和 `x_test` 中的每一行都是一个文档的词向量表示。\n",
    "- 每一列都是词向量的一个维度，这些维度共同构成了文档的高维向量表示。\n",
    "\n",
    "如果您使用的是Word2Vec模型来生成这些向量，那么每个文档的向量表示可能是通过以下方法之一得到的：\n",
    "\n",
    "- **平均词向量**：对于每个文档，计算其中所有单词的词向量的平均值，以得到文档的向量表示。\n",
    "- **TF-IDF 加权和**：使用 TF-IDF 权重来加权每个单词的词向量，然后求和得到文档的向量表示。\n",
    "- **其他方法**：如Doc2Vec，它直接学习文档的向量表示。\n",
    "\n",
    "请注意，具体的转换方法取决于您的 `get_features_by_word2vec()` 函数的实现细节。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(binary=True, decode_error='ignore', max_features=1000,\n",
      "                ngram_range=(3, 3), stop_words='english', strip_accents='ascii',\n",
      "                token_pattern='\\\\b\\\\w+\\\\b')\n",
      "CountVectorizer(binary=True, decode_error='ignore', ngram_range=(3, 3),\n",
      "                stop_words='english', strip_accents='ascii',\n",
      "                token_pattern='\\\\b\\\\w+\\\\b',\n",
      "                vocabulary={'000 bonus caller': 0, '000 cash await': 1,\n",
      "                            '02 06 03': 2, '03 2nd attempt': 3, '06 03 2nd': 4,\n",
      "                            '08702840625 comuk 220': 5,\n",
      "                            '08707509020 just 20p': 6,\n",
      "                            '08712300220 quoting claim': 7,\n",
      "                            '087187262701 50gbp mtmsg18': 8,\n",
      "                            '09050090044 toclaim s...9,\n",
      "                            '09061209465 c suprman': 10,\n",
      "                            '09061221066 fromm landline': 11, '1 2 3': 12,\n",
      "                            '1 2 price': 13, '1 50 msg': 14, '1 50 sp': 15,\n",
      "                            '1 new message': 16, '1 new voicemail': 17,\n",
      "                            '1 wife called': 18, '1 www 4': 19,\n",
      "                            '1 year special': 20, '10 000 cash': 21,\n",
      "                            '10 txtauction txt': 22, '100 free text': 23,\n",
      "                            '100 wkly draw': 24, '1000 cash 2000': 25,\n",
      "                            '1000 cash 5000': 26, '1000 prize guaranteed': 27,\n",
      "                            '10am 7pm cost': 28, '10p min 2': 29, ...})\n"
     ]
    }
   ],
   "source": [
    "def get_features_by_ngram():\n",
    "    global max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 ngram_range=(3, 3),\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1,\n",
    "                                 token_pattern=r'\\b\\w+\\b',\n",
    "                                 binary=True)\n",
    "    print(vectorizer)\n",
    "    x_train=vectorizer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    vocabulary=vectorizer.vocabulary_\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 ngram_range=(3, 3),\n",
    "                                 strip_accents='ascii',\n",
    "                                 vocabulary=vocabulary,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,binary=True,\n",
    "                                 token_pattern=r'\\b\\w+\\b',\n",
    "                                 min_df=1 )\n",
    "    print(vectorizer)\n",
    "    x_test=vectorizer.fit_transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    x_train=transformer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    x_test=transformer.transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_features_by_ngram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3344 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  990  991  992  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3339  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3340  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3341  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3342  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3343  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      993       994  995  996  997  998  999  \n",
       "0     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...       ...  ...  ...  ...  ...  ...  \n",
       "3339  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "3340  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "3341  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "3342  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "3343  0.0  0.229503  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[3344 rows x 1000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3　模型训练与验证\n",
    "\n",
    "## 8.3.1　朴素贝叶斯算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nb_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print(\"NB and wordbag\")\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def do_nb_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"NB and doc2vec\")\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def do_nb_word2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"NB and word2vec\")\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_nb_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"NB and doc2vec\")\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def do_nb_word2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"NB and word2vec\")\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.2　支持向量机算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_svm_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print(\"SVM and wordbag\")\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "def do_svm_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"SVM and doc2vec\")\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def do_svm_word2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"SVM and word2vec\")\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.3　XGBoost算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_xgboost_word2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"xgboost and word2vec\")\n",
    "    xgb_model = xgb.XGBClassifier().fit(x_train, y_train)\n",
    "    y_pred = xgb_model.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def do_xgboost_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print(\"xgboost and wordbag\")\n",
    "    xgb_model = xgb.XGBClassifier().fit(x_train, y_train)\n",
    "    y_pred = xgb_model.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.4　深度学习算法之MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_dnn_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print(\"MLP and wordbag\")\n",
    "\n",
    "    # Building deep neural network\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    print(clf)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def do_dnn_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"MLP and doc2vec\")\n",
    "    global max_features\n",
    "    # Building deep neural network\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    print(clf)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def do_dnn_word2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"MLP and word2vec\")\n",
    "    global max_features\n",
    "    # Building deep neural network\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    print(clf)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def do_dnn_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print(\"MLP and wordbag\")\n",
    "    global max_features\n",
    "    # Building deep neural network\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    print(clf)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rf_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"rf and doc2vec\")\n",
    "    clf = RandomForestClassifier(n_estimators=10)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def do_rf_word2vec(x_train, x_test, y_train, y_test):\n",
    "    print(\"rf and word2vec\")\n",
    "    clf = RandomForestClassifier(n_estimators=50)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cnn_wordbag(trainX, testX, trainY, testY):\n",
    "    global max_document_length\n",
    "    print(\"CNN and tf\")\n",
    "\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,max_document_length], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=1000000, output_dim=128)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_cnn_doc2vec_2d(trainX, testX, trainY, testY):\n",
    "    print(\"CNN and doc2vec 2d\")\n",
    "\n",
    "    trainX = trainX.reshape([-1, max_features, max_document_length, 1])\n",
    "    testX = testX.reshape([-1, max_features, max_document_length, 1])\n",
    "\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None, max_features, max_document_length, 1], name='input')\n",
    "    network = conv_2d(network, 16, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 128, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 256, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.01,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit({'input': trainX}, {'target': trainY}, n_epoch=20,\n",
    "               validation_set=({'input': testX}, {'target': testY}),\n",
    "               snapshot_step=100, show_metric=True, run_id='review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cnn_word2vec_2d(trainX, testX, trainY, testY):\n",
    "    global max_features\n",
    "    global max_document_length\n",
    "    print(\"CNN and word2vec2d\")\n",
    "    y_test = testY\n",
    "    #trainX = pad_sequences(trainX, maxlen=max_features, value=0.)\n",
    "    #testX = pad_sequences(testX, maxlen=max_features, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,max_document_length,max_features,1], name='input')\n",
    "\n",
    "    network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 64, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 128, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 256, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.01,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True,run_id=\"sms\")\n",
    "\n",
    "    y_predict_list = model.predict(testX)\n",
    "    print(y_predict_list)\n",
    "\n",
    "    y_predict = []\n",
    "    for i in y_predict_list:\n",
    "        print(i[0])\n",
    "        if i[0] > 0.5:\n",
    "            y_predict.append(0)\n",
    "        else:\n",
    "            y_predict.append(1)\n",
    "\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_features_by_word2vec_cnn_1d():\n",
    "    global  max_features\n",
    "    global word2ver_bin\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    x_train=cleanText(x_train)\n",
    "    x_test=cleanText(x_test)\n",
    "\n",
    "    x=x_train+x_test\n",
    "    cores=multiprocessing.cpu_count()\n",
    "\n",
    "    if os.path.exists(word2ver_bin):\n",
    "        print(\"Find cache file %s\" % word2ver_bin)\n",
    "        model=gensim.models.Word2Vec.load(word2ver_bin)\n",
    "    else:\n",
    "        model=gensim.models.Word2Vec(size=max_features, window=10, min_count=1, iter=60, workers=cores)\n",
    "\n",
    "        model.build_vocab(x)\n",
    "\n",
    "        model.train(x, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        # model.save((word2ver_bin)\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    x_train= np.concatenate([buildWordVector(model,z, max_features) for z in x_train])\n",
    "    x_train = min_max_scaler.fit_transform(x_train)\n",
    "    x_test= np.concatenate([buildWordVector(model,z, max_features) for z in x_test])\n",
    "    x_test = min_max_scaler.transform(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_features_by_word2vec_cnn_2d():\n",
    "    global max_features\n",
    "    global max_document_length\n",
    "    global word2ver_bin\n",
    "\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    x_train_vecs=[]\n",
    "    x_test_vecs=[]\n",
    "\n",
    "    x_train=cleanText(x_train)\n",
    "    x_test=cleanText(x_test)\n",
    "\n",
    "    x=x_train+x_test\n",
    "    cores=multiprocessing.cpu_count()\n",
    "\n",
    "    if os.path.exists(word2ver_bin):\n",
    "        print(\"Find cache file %s\" % word2ver_bin)\n",
    "        model=gensim.models.Word2Vec.load(word2ver_bin)\n",
    "    else:\n",
    "        model=gensim.models.Word2Vec(size=max_features, window=10, min_count=1, iter=60, workers=cores)\n",
    "\n",
    "        model.build_vocab(x)\n",
    "\n",
    "        model.train(x, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        # model.save((word2ver_bin)\n",
    "\n",
    "\n",
    "    #x_train_vec=np.zeros((max_document_length,max_features))\n",
    "    #x_test_vec=np.zeros((max_document_length, max_features))\n",
    "    \"\"\"\n",
    "    x_train= np.concatenate([buildWordVector(model,z, max_features) for z in x_train])\n",
    "    x_train = min_max_scaler.fit_transform(x_train)\n",
    "    x_test= np.concatenate([buildWordVector(model,z, max_features) for z in x_test])\n",
    "    x_test = min_max_scaler.transform(x_test)\n",
    "    vec += imdb_w2v[word].reshape((1, size))\n",
    "    \"\"\"\n",
    "    #x_train = np.concatenate([buildWordVector_2d(model, z, max_features) for z in x_train])\n",
    "    x_all=np.zeros((1,max_features))\n",
    "    for sms in x_train:\n",
    "        sms=sms[:max_document_length]\n",
    "        #print sms\n",
    "        x_train_vec = np.zeros((max_document_length, max_features))\n",
    "        for i,w in enumerate(sms):\n",
    "            vec=model[w].reshape((1, max_features))\n",
    "            x_train_vec[i-1]=vec.copy()\n",
    "            #x_all=np.concatenate((x_all,vec))\n",
    "        x_train_vecs.append(x_train_vec)\n",
    "        #print x_train_vec.shape\n",
    "    for sms in x_test:\n",
    "        sms=sms[:max_document_length]\n",
    "        #print sms\n",
    "        x_test_vec = np.zeros((max_document_length, max_features))\n",
    "        for i,w in enumerate(sms):\n",
    "            vec=model[w].reshape((1, max_features))\n",
    "            x_test_vec[i-1]=vec.copy()\n",
    "            #x_all.append(vec)\n",
    "        x_test_vecs.append(x_test_vec)\n",
    "\n",
    "    #print x_train\n",
    "    #print x_all\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    print(\"fix min_max_scaler\")\n",
    "    x_train_2d=np.concatenate([z for z in x_train_vecs])\n",
    "    min_max_scaler.fit(x_train_2d)\n",
    "\n",
    "    x_train=np.concatenate([min_max_scaler.transform(i) for i in x_train_vecs])\n",
    "    x_test=np.concatenate([min_max_scaler.transform(i) for i in x_test_vecs])\n",
    "\n",
    "    x_train=x_train.reshape([-1, max_document_length, max_features, 1])\n",
    "    x_test = x_test.reshape([-1, max_document_length, max_features, 1])\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cnn_word2vec_2d_345(trainX, testX, trainY, testY):\n",
    "    global max_features\n",
    "    global max_document_length\n",
    "    print(\"CNN and word2vec_2d_345\")\n",
    "    y_test = testY\n",
    "\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,max_document_length,max_features,1], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=1, output_dim=128,validate_indices=False)\n",
    "    branch1 = conv_2d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_2d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_2d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool_2d(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"sms\")\n",
    "\n",
    "    y_predict_list = model.predict(testX)\n",
    "    print(y_predict_list)\n",
    "\n",
    "    y_predict = []\n",
    "    for i in y_predict_list:\n",
    "        print(i[0])\n",
    "        if i[0] > 0.5:\n",
    "            y_predict.append(0)\n",
    "        else:\n",
    "            y_predict.append(1)\n",
    "\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cnn_word2vec(trainX, testX, trainY, testY):\n",
    "    global max_features\n",
    "    print(\"CNN and word2vec\")\n",
    "    y_test = testY\n",
    "    #trainX = pad_sequences(trainX, maxlen=max_features, value=0.)\n",
    "    #testX = pad_sequences(testX, maxlen=max_features, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,max_features], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=1000000, output_dim=128,validate_indices=False)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"sms\")\n",
    "\n",
    "    y_predict_list = model.predict(testX)\n",
    "    print(y_predict_list)\n",
    "\n",
    "    y_predict = []\n",
    "    for i in y_predict_list:\n",
    "        print(i[0])\n",
    "        if i[0] > 0.5:\n",
    "            y_predict.append(0)\n",
    "        else:\n",
    "            y_predict.append(1)\n",
    "\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cnn_doc2vec(trainX, testX, trainY, testY):\n",
    "    global max_features\n",
    "    print(\"CNN and doc2vec\")\n",
    "\n",
    "    #trainX = pad_sequences(trainX, maxlen=max_features, value=0.)\n",
    "    #testX = pad_sequences(testX, maxlen=max_features, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,max_features], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=1000000, output_dim=128,validate_indices=False)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_rnn_wordbag(trainX, testX, trainY, testY):\n",
    "    global max_document_length\n",
    "    print(\"RNN and wordbag\")\n",
    "    y_test=testY\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Network building\n",
    "    net = tflearn.input_data([None, max_document_length])\n",
    "    net = tflearn.embedding(net, input_dim=10240000, output_dim=128)\n",
    "    net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='categorical_crossentropy')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "              batch_size=10,run_id=\"sms\",n_epoch=5)\n",
    "\n",
    "    y_predict_list = model.predict(testX)\n",
    "    print(y_predict_list)\n",
    "\n",
    "    y_predict = []\n",
    "    for i in y_predict_list:\n",
    "        print(i[0])\n",
    "        if i[0] > 0.5:\n",
    "            y_predict.append(0)\n",
    "        else:\n",
    "            y_predict.append(1)\n",
    "\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_rnn_word2vec(trainX, testX, trainY, testY):\n",
    "    global max_features\n",
    "    print(\"RNN and wordbag\")\n",
    "\n",
    "    trainX = pad_sequences(trainX, maxlen=max_features, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_features, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Network building\n",
    "    net = tflearn.input_data([None, max_features])\n",
    "    net = tflearn.embedding(net, input_dim=10240000, output_dim=128)\n",
    "    net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='categorical_crossentropy')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "              batch_size=10,run_id=\"sms\",n_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集中比较\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = get_features_by_doc2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB and doc2vec\n",
      "0.9672645739910314\n",
      "[[1871   53]\n",
      " [  20  286]]\n"
     ]
    }
   ],
   "source": [
    "do_nb_doc2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf and doc2vec\n",
      "0.9775784753363229\n",
      "[[1922    2]\n",
      " [  48  258]]\n"
     ]
    }
   ],
   "source": [
    "do_rf_doc2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM and doc2vec\n",
      "0.9838565022421525\n",
      "[[1919    5]\n",
      " [  31  275]]\n"
     ]
    }
   ],
   "source": [
    "do_svm_doc2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP and doc2vec\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "0.973542600896861\n",
      "[[1895   29]\n",
      " [  30  276]]\n"
     ]
    }
   ],
   "source": [
    "do_dnn_doc2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN and doc2vec 2d\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3344000 into shape (1000,160,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-0dc515734298>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdo_cnn_doc2vec_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-b357d6d06162>\u001b[0m in \u001b[0;36mdo_cnn_doc2vec_2d\u001b[1;34m(trainX, testX, trainY, testY)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CNN and doc2vec 2d\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mtrainX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_document_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mtestX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_document_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 3344000 into shape (1000,160,1)"
     ]
    }
   ],
   "source": [
    "do_cnn_doc2vec_2d(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test=get_features_by_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB and word2vec\n",
      "0.9726457399103139\n",
      "[[1892   38]\n",
      " [  23  277]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1930\n",
      "           1       0.88      0.92      0.90       300\n",
      "\n",
      "    accuracy                           0.97      2230\n",
      "   macro avg       0.93      0.95      0.94      2230\n",
      "weighted avg       0.97      0.97      0.97      2230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do_nb_word2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP and word2vec\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1930\n",
      "           1       0.91      0.94      0.93       300\n",
      "\n",
      "    accuracy                           0.98      2230\n",
      "   macro avg       0.95      0.96      0.96      2230\n",
      "weighted avg       0.98      0.98      0.98      2230\n",
      "\n",
      "[[1903   27]\n",
      " [  18  282]]\n"
     ]
    }
   ],
   "source": [
    "do_dnn_word2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM and word2vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1930\n",
      "           1       0.99      0.93      0.96       300\n",
      "\n",
      "    accuracy                           0.99      2230\n",
      "   macro avg       0.99      0.96      0.98      2230\n",
      "weighted avg       0.99      0.99      0.99      2230\n",
      "\n",
      "0.9896860986547085\n",
      "[[1928    2]\n",
      " [  21  279]]\n"
     ]
    }
   ],
   "source": [
    "do_svm_word2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf and word2vec\n",
      "0.9825112107623318\n",
      "[[1928    2]\n",
      " [  37  263]]\n"
     ]
    }
   ],
   "source": [
    "do_rf_word2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost and word2vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1930\n",
      "           1       0.98      0.92      0.95       300\n",
      "\n",
      "    accuracy                           0.99      2230\n",
      "   macro avg       0.98      0.96      0.97      2230\n",
      "weighted avg       0.99      0.99      0.99      2230\n",
      "\n",
      "[[1925    5]\n",
      " [  25  275]]\n"
     ]
    }
   ],
   "source": [
    "do_xgboost_word2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN and wordbag\n",
      "WARNING:tensorflow:From d:\\Anaconda\\lib\\site-packages\\tflearn\\layers\\recurrent.py:69: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[10240000,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Embedding_1/W/Initializer/truncated_normal/TruncatedNormal (defined at d:\\Anaconda\\lib\\site-packages\\tflearn\\variables.py:63) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nOriginal stack trace for 'Embedding_1/W/Initializer/truncated_normal/TruncatedNormal':\n  File \"d:\\Anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"d:\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"d:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n    app.start()\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"d:\\Anaconda\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n    self._run_once()\n  File \"d:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n    handle._run()\n  File \"d:\\Anaconda\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 687, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 740, in _run_callback\n    ret = callback()\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 821, in inner\n    self.ctx_run(self.run)\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 782, in run\n    yielded = self.gen.send(value)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-86-ca6ea112e6d4>\", line 1, in <module>\n    do_rnn_word2vec(x_train, x_test, y_train, y_test)\n  File \"<ipython-input-79-2d4c1fd1f1e6>\", line 13, in do_rnn_word2vec\n    net = tflearn.embedding(net, input_dim=10240000, output_dim=128)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\layers\\embedding_ops.py\", line 58, in embedding\n    restore=restore)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\vendor\\arg_scope.py\", line 187, in func_with_args\n    return func(*args, **current_args)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\variables.py\", line 63, in variable\n    validate_shape=validate_shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1572, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1315, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 569, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 521, in _true_getter\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 940, in _get_single_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 260, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 221, in _variable_v1_call\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 199, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2613, in default_variable_creator\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1668, in __init__\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1798, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 909, in <lambda>\n    partition_info=partition_info)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py\", line 355, in __call__\n    shape, self.mean, self.stddev, dtype, seed=self.seed)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 196, in truncated_normal\n    shape_tensor, dtype, seed=seed1, seed2=seed2)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\", line 920, in truncated_normal\n    name=name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n    op_def=op_def)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10240000,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node Embedding_1/W/Initializer/truncated_normal/TruncatedNormal}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-ca6ea112e6d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdo_rnn_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-2d4c1fd1f1e6>\u001b[0m in \u001b[0;36mdo_rnn_word2vec\u001b[1;34m(trainX, testX, trainY, testY)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_verbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n\u001b[0;32m     22\u001b[0m               batch_size=10,run_id=\"sms\",n_epoch=5)\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, network, clip_gradients, tensorboard_verbose, tensorboard_dir, checkpoint_path, best_checkpoint_path, max_checkpoints, session, best_val_accuracy)\u001b[0m\n\u001b[0;32m     63\u001b[0m                                \u001b[0mmax_checkpoints\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_checkpoints\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                                \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                                best_val_accuracy=best_val_accuracy)\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_ops, graph, clip_gradients, tensorboard_dir, tensorboard_verbose, checkpoint_path, best_checkpoint_path, max_checkpoints, keep_checkpoint_every_n_hours, random_seed, session, best_val_accuracy)\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[1;31m# Fix for re-using sessions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;31m#initialize_uninit_variables(self.session)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 958\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1181\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1384\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10240000,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Embedding_1/W/Initializer/truncated_normal/TruncatedNormal (defined at d:\\Anaconda\\lib\\site-packages\\tflearn\\variables.py:63) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nOriginal stack trace for 'Embedding_1/W/Initializer/truncated_normal/TruncatedNormal':\n  File \"d:\\Anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"d:\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"d:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n    app.start()\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"d:\\Anaconda\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n    self._run_once()\n  File \"d:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n    handle._run()\n  File \"d:\\Anaconda\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 687, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 740, in _run_callback\n    ret = callback()\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 821, in inner\n    self.ctx_run(self.run)\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 782, in run\n    yielded = self.gen.send(value)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-86-ca6ea112e6d4>\", line 1, in <module>\n    do_rnn_word2vec(x_train, x_test, y_train, y_test)\n  File \"<ipython-input-79-2d4c1fd1f1e6>\", line 13, in do_rnn_word2vec\n    net = tflearn.embedding(net, input_dim=10240000, output_dim=128)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\layers\\embedding_ops.py\", line 58, in embedding\n    restore=restore)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\vendor\\arg_scope.py\", line 187, in func_with_args\n    return func(*args, **current_args)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\variables.py\", line 63, in variable\n    validate_shape=validate_shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1572, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1315, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 569, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 521, in _true_getter\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 940, in _get_single_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 260, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 221, in _variable_v1_call\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 199, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2613, in default_variable_creator\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1668, in __init__\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1798, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 909, in <lambda>\n    partition_info=partition_info)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py\", line 355, in __call__\n    shape, self.mean, self.stddev, dtype, seed=self.seed)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 196, in truncated_normal\n    shape_tensor, dtype, seed=seed1, seed2=seed2)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\", line 920, in truncated_normal\n    name=name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n    op_def=op_def)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "do_rnn_word2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find cache file smsword2ver.bin\n",
      "CNN and word2vec\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1000000\n  }\n  dim {\n    size: 128\n  }\n}\nfloat_val: 0\n\n\t [[node Embedding_2/W/Adam/Initializer/zeros (defined at d:\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py:716) ]]\n\nOriginal stack trace for 'Embedding_2/W/Adam/Initializer/zeros':\n  File \"d:\\Anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"d:\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"d:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n    app.start()\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"d:\\Anaconda\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n    self._run_once()\n  File \"d:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n    handle._run()\n  File \"d:\\Anaconda\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 687, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 740, in _run_callback\n    ret = callback()\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 821, in inner\n    self.ctx_run(self.run)\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 782, in run\n    yielded = self.gen.send(value)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-87-b2bc8fb4265d>\", line 2, in <module>\n    do_cnn_word2vec(x_train, x_test, y_train, y_test)\n  File \"<ipython-input-76-e6855bf2f79f>\", line 25, in do_cnn_word2vec\n    model = tflearn.DNN(network, tensorboard_verbose=0)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\models\\dnn.py\", line 65, in __init__\n    best_val_accuracy=best_val_accuracy)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py\", line 131, in __init__\n    clip_gradients)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py\", line 716, in initialize_training_ops\n    name=\"apply_grad_op_\" + str(i))\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 597, in apply_gradients\n    self._create_slots(var_list)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 138, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 1158, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 197, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 171, in create_slot_with_initializer\n    dtype)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 73, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1572, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1315, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 569, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 521, in _true_getter\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 940, in _get_single_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 260, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 221, in _variable_v1_call\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 199, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2613, in default_variable_creator\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1668, in __init__\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1798, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 909, in <lambda>\n    partition_info=partition_info)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py\", line 114, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2747, in wrapped\n    tensor = fun(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2806, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 239, in fill\n    result = gen_array_ops.fill(dims, value, name=name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3412, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n    op_def=op_def)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1000000\n  }\n  dim {\n    size: 128\n  }\n}\nfloat_val: 0\n\n\t [[{{node Embedding_2/W/Adam/Initializer/zeros}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-b2bc8fb4265d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mget_features_by_word2vec_cnn_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdo_cnn_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-76-e6855bf2f79f>\u001b[0m in \u001b[0;36mdo_cnn_word2vec\u001b[1;34m(trainX, testX, trainY, testY)\u001b[0m\n\u001b[0;32m     23\u001b[0m                          loss='categorical_crossentropy', name='target')\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_verbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     model.fit(trainX, trainY,\n\u001b[0;32m     27\u001b[0m               \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, network, clip_gradients, tensorboard_verbose, tensorboard_dir, checkpoint_path, best_checkpoint_path, max_checkpoints, session, best_val_accuracy)\u001b[0m\n\u001b[0;32m     63\u001b[0m                                \u001b[0mmax_checkpoints\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_checkpoints\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                                \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                                best_val_accuracy=best_val_accuracy)\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_ops, graph, clip_gradients, tensorboard_dir, tensorboard_verbose, checkpoint_path, best_checkpoint_path, max_checkpoints, keep_checkpoint_every_n_hours, random_seed, session, best_val_accuracy)\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[1;31m# Fix for re-using sessions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;31m#initialize_uninit_variables(self.session)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 958\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1181\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1384\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1000000\n  }\n  dim {\n    size: 128\n  }\n}\nfloat_val: 0\n\n\t [[node Embedding_2/W/Adam/Initializer/zeros (defined at d:\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py:716) ]]\n\nOriginal stack trace for 'Embedding_2/W/Adam/Initializer/zeros':\n  File \"d:\\Anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"d:\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"d:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n    app.start()\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"d:\\Anaconda\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n    self._run_once()\n  File \"d:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n    handle._run()\n  File \"d:\\Anaconda\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 687, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 740, in _run_callback\n    ret = callback()\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 821, in inner\n    self.ctx_run(self.run)\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 782, in run\n    yielded = self.gen.send(value)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"d:\\Anaconda\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"d:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"d:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-87-b2bc8fb4265d>\", line 2, in <module>\n    do_cnn_word2vec(x_train, x_test, y_train, y_test)\n  File \"<ipython-input-76-e6855bf2f79f>\", line 25, in do_cnn_word2vec\n    model = tflearn.DNN(network, tensorboard_verbose=0)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\models\\dnn.py\", line 65, in __init__\n    best_val_accuracy=best_val_accuracy)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py\", line 131, in __init__\n    clip_gradients)\n  File \"d:\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py\", line 716, in initialize_training_ops\n    name=\"apply_grad_op_\" + str(i))\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 597, in apply_gradients\n    self._create_slots(var_list)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 138, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 1158, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 197, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 171, in create_slot_with_initializer\n    dtype)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 73, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1572, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1315, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 569, in get_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 521, in _true_getter\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 940, in _get_single_variable\n    aggregation=aggregation)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 260, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 221, in _variable_v1_call\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 199, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2613, in default_variable_creator\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1668, in __init__\n    shape=shape)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1798, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 909, in <lambda>\n    partition_info=partition_info)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py\", line 114, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2747, in wrapped\n    tensor = fun(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2806, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 239, in fill\n    result = gen_array_ops.fill(dims, value, name=name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3412, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 744, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3485, in _create_op_internal\n    op_def=op_def)\n  File \"d:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = get_features_by_word2vec_cnn_1d()\n",
    "do_cnn_word2vec(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN and word2vec2d\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-4bc6fd9febca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdo_cnn_word2vec_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-69-6da9f60dce63>\u001b[0m in \u001b[0;36mdo_cnn_word2vec_2d\u001b[1;34m(trainX, testX, trainY, testY)\u001b[0m\n\u001b[0;32m     30\u001b[0m     model.fit(trainX, trainY,\n\u001b[0;32m     31\u001b[0m               \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m               show_metric=True,run_id=\"sms\")\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0my_predict_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# TODO: check memory impact for large data and multiple optimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         feed_dict = feed_dict_builder(X_inputs, Y_targets, self.inputs,\n\u001b[1;32m--> 184\u001b[1;33m                                       self.targets)\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[0mfeed_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mval_feed_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tflearn\\utils.py\u001b[0m in \u001b[0;36mfeed_dict_builder\u001b[1;34m(X, Y, net_inputs, net_targets)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnet_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# If a dict is provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = get_features_by_word2vec_cnn_2d()\n",
    "do_cnn_word2vec_2d(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'do_cnn_word2vec_2d_345' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-8a94a4f7e9b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdo_cnn_word2vec_2d_345\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'do_cnn_word2vec_2d_345' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = get_features_by_word2vec_cnn_1d_log()\n",
    "do_cnn_word2vec_2d_345(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordbag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(decode_error='ignore', max_features=1000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '02': 2, '03': 3, '04': 4,\n",
      "                            '0800': 5, '08000839402': 6, '08000930705': 7,\n",
      "                            '08718720201': 8, '10': 9, '100': 10, '1000': 11,\n",
      "                            '10p': 12, '11': 13, '11mths': 14, '12': 15,\n",
      "                            '12hrs': 16, '150': 17, '150p': 18, '150ppm': 19,\n",
      "                            '16': 20, '18': 21, '1st': 22, '20': 23, '200': 24,\n",
      "                            '2000': 25, '2003': 26, '2004': 27, '250': 28,\n",
      "                            '25p': 29, ...})\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = get_features_by_wordbag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB and wordbag\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.76      0.86      1928\n",
      "           1       0.38      0.94      0.55       302\n",
      "\n",
      "    accuracy                           0.79      2230\n",
      "   macro avg       0.69      0.85      0.70      2230\n",
      "weighted avg       0.91      0.79      0.82      2230\n",
      "\n",
      "[[1474  454]\n",
      " [  18  284]]\n"
     ]
    }
   ],
   "source": [
    "do_nb_wordbag(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP and wordbag\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1928\n",
      "           1       0.94      0.91      0.92       302\n",
      "\n",
      "    accuracy                           0.98      2230\n",
      "   macro avg       0.96      0.95      0.96      2230\n",
      "weighted avg       0.98      0.98      0.98      2230\n",
      "\n",
      "[[1909   19]\n",
      " [  26  276]]\n"
     ]
    }
   ],
   "source": [
    "do_dnn_wordbag(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost and wordbag\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1928\n",
      "           1       0.95      0.86      0.90       302\n",
      "\n",
      "    accuracy                           0.97      2230\n",
      "   macro avg       0.96      0.93      0.94      2230\n",
      "weighted avg       0.97      0.97      0.97      2230\n",
      "\n",
      "[[1914   14]\n",
      " [  42  260]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "do_xgboost_wordbag(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN and wordbag\n"
     ]
    }
   ],
   "source": [
    "do_rnn_wordbag(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = get_features_by_ngram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =get_features_by_tf()\n",
    "x_train, x_test, y_train, y_test=get_features_by_wordbag_tfidf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
