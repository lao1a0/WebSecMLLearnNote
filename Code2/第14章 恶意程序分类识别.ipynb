{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "# from tensorflow.contrib import learn\n",
    "import gensim\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from gensim.models import Doc2Vec\n",
    "# from gensim.models.doc2vec import Doc2Vec,LabeledSentence\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tflearn.layers.recurrent import bidirectional_rnn, BasicLSTMCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.1　数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_from_dir(dir):\n",
    "    import glob\n",
    "    files=glob.glob(dir)\n",
    "    result = []\n",
    "    for file in files:\n",
    "        #print(\"Load file %s\" % file\n",
    "        with open(file) as f:\n",
    "            lines=f.readlines()\n",
    "            lines_to_line=\" \".join(lines)\n",
    "            lines_to_line = re.sub(r\"[APT|Crypto|Locker|Zeus]\", ' ', lines_to_line,flags=re.I)\n",
    "            result.append(lines_to_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    #/data/malware/MalwareTrainingSets-master/trainingSets\n",
    "    malware_class=['APT1','Crypto','Locker','Zeus']\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i,family in enumerate(malware_class):\n",
    "        dir=\"../Data/malware/MalwareTrainingSets-master/trainingSets/%s/*\" % family\n",
    "        print(\"Load files from %s index %d\" % (dir,i))\n",
    "        v=load_files_from_dir(dir)\n",
    "        x+=v\n",
    "        y+=[i]*len(v)\n",
    "    print(\"Loaded files %d\" % len(x))\n",
    "    return x,y\n",
    "\n",
    "def get_feature_pe_picture():\n",
    "    #加载原始文件\n",
    "    x,y=load_files()\n",
    "    max_features=1024\n",
    "    vectorizer = CountVectorizer(\n",
    "            decode_error='ignore',\n",
    "            ngram_range=(2, 2),\n",
    "            strip_accents='ascii',\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            max_df=1.0,\n",
    "            min_df=1,\n",
    "            dtype=np.int,\n",
    "            token_pattern=r'\\b\\w+\\b',\n",
    "            binary=False)\n",
    "    # print vectorizer\n",
    "    x=vectorizer.fit_transform(x)\n",
    "    #非常重要 稀疏矩阵转换成矩阵\n",
    "    x=x.toarray()\n",
    "    x_pic = []\n",
    "    for i in range(4762):\n",
    "        #将形状为(1024，1)的向量转化成（32，32）的矩阵\n",
    "        pic=np.reshape(x[i],(32,32,1))\n",
    "        x_pic.append(pic)\n",
    "        #save_image(pic,i)\n",
    "    #随机分配训练和测试集合\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_pic, y, test_size=0.4)\n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_xgboost(x_train, x_test, y_train, y_test):\n",
    "    xgb_model = xgb.XGBClassifier().fit(x_train, y_train)\n",
    "    y_pred = xgb_model.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "def do_svm(x_train, x_test, y_train, y_test):\n",
    "    from sklearn.svm import SVC\n",
    "    clf = svm.SVC(kernel='linear', C=1.0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "def do_mlp(x_train, x_test, y_train, y_test):\n",
    "\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (10, 4),\n",
    "                        random_state = 1)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def get_feature_text():\n",
    "    x,y=load_files()\n",
    "    max_features=1000\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "            decode_error='ignore',\n",
    "            ngram_range=(2, 2),\n",
    "            strip_accents='ascii',\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            max_df=1.0,\n",
    "            min_df=1,\n",
    "            token_pattern=r'\\b\\w+\\b',\n",
    "            binary=True)\n",
    "    # print vectorizer\n",
    "    x=vectorizer.fit_transform(x)\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    x = transformer.fit_transform(x)\n",
    "\n",
    "    # 非常重要 稀疏矩阵转换成矩阵\n",
    "    x = x.toarray()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def save_image(data,index):\n",
    "    from PIL import Image\n",
    "    new_im = Image.fromarray(data)\n",
    "    new_im.save(\"../Data/malware/%s.bmp\" % index)\n",
    "\n",
    "\n",
    "\n",
    "def do_cnn(trainX, testX, trainY, testY):\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=4)\n",
    "    testY = to_categorical(testY, nb_classes=4)\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None, 32, 32,1], name='input')\n",
    "    network = conv_2d(network, 16, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 16, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 16, activation='tanh')\n",
    "    network = dropout(network, 0.1)\n",
    "    network = fully_connected(network, 16, activation='tanh')\n",
    "    network = dropout(network, 0.1)\n",
    "    network = fully_connected(network, 4, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.01,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY, n_epoch=10, validation_set=(testX, testY),show_metric=True, run_id=\"malware\")\n",
    "\n",
    "def do_cnn_1d(trainX, testX, trainY, testY):\n",
    "\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=4)\n",
    "    testY = to_categorical(testY, nb_classes=4)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,1000], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=1000000, output_dim=128,validate_indices=False)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 4, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"malware\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"Hello malware\"\n",
    "\n",
    "    # print(\"text feature and cnn\"\n",
    "    x_train, x_test, y_train, y_test=get_feature_text()\n",
    "    do_cnn_1d(x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    print(\"text feature and xgboost\"\n",
    "    x_train, x_test, y_train, y_test=get_feature_text()\n",
    "    do_xgboost(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    print(\"text feature and svm\"\n",
    "    x_train, x_test, y_train, y_test=get_feature_text()\n",
    "    do_svm(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    print(\"text feature and mlp\"\n",
    "    x_train, x_test, y_train, y_test=get_feature_text()\n",
    "    do_mlp(x_train, x_test, y_train, y_test)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
